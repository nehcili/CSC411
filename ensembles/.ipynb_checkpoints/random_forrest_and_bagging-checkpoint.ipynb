{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging with Random Forest\n",
    "The goal of this project is to implement random forrest with gini measure as the splitting loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modified Decision Tree\n",
    "We start with an implementation of the decision tree with gini splitting loss function\n",
    "This is essentially the same code as the one in ../2_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gini_indx: np.array --> float\n",
    "# group is a list of integers, where each integer represents a class type\n",
    "def gini_index(groups):\n",
    "    # count all samples at split point\n",
    "    n_instances = sum([len(group) for group in groups])\n",
    "    \n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 1.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "\n",
    "        # score the group based on the score for each class\n",
    "        unique, counts = np.unique(group, return_counts=True)\n",
    "        #print(counts)\n",
    "        score = np.sum((counts / size) ** 2)\n",
    " \n",
    "        # weight the group score by its relative size\n",
    "        gini -= score * (size / n_instances)\n",
    "        \n",
    "    return gini\n",
    " \n",
    "# test Gini values\n",
    "# print(gini_index(np.array([[1, 0], [1, 0, 1]])))\n",
    "# print(gini_index(np.array([[0, 0], [1, 1, 1]])))\n",
    "\n",
    "# try_split: list of int, float, np.array --> np.array, np.array\n",
    "# split data using the feature in data[:, index] \n",
    "# if feature < value => that row in data is in group 1 (left_index)\n",
    "# else that row in data is in group 2 (right_index)\n",
    "# returns the index of the two groups in data\n",
    "def try_split(index, value, data):\n",
    "    feature = data[:, index]\n",
    "    \n",
    "    left_index = np.where(feature < value)[0]\n",
    "    right_index = np.where(feature >= value)[0]\n",
    "    \n",
    "    return left_index, right_index       \n",
    "\n",
    "# get_binary_split: np.array, np.array, np.array --> set\n",
    "# perform greedy split of data. Trained with target labels\n",
    "# the features are supplied (randomly in the random forrest)\n",
    "# returns a set containing: gini, best index (i.e. feature to split), value at which split occurs, and\n",
    "# the best left and right, b_left, b_right split.\n",
    "def get_binary_split(data, labels, features):\n",
    "    b_index, b_gini, b_value, b_left, b_right = np.nan, 1, np.nan, None, None\n",
    "    \n",
    "    for index in features:\n",
    "        for row in data:\n",
    "            left_index, right_index = try_split(index, row[index], data)\n",
    "            gini = gini_index([labels[left_index], labels[right_index]])\n",
    "            \n",
    "            if gini < b_gini:\n",
    "                b_index, b_gini, b_value, b_left, b_right = index, gini, row[index], left_index, right_index\n",
    "    \n",
    "    return {'gini': b_gini, 'index':b_index, 'value':b_value, 'left':b_left, 'right':b_right}  \n",
    "\n",
    "class DecisionTree(object):\n",
    "    \n",
    "    # default depth = 1 and no features is passed\n",
    "    def __init__(self, depth=1, features=np.array([])):\n",
    "        self.depth = depth\n",
    "        self.node = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.features = features\n",
    "        \n",
    "    def most_freq(self, arr):\n",
    "        unique, counts = np.unique(arr, return_counts=True)\n",
    "        ind = np.argmax(counts)\n",
    "        return unique[ind]\n",
    "    \n",
    "    # data is an np.array of size N times D\n",
    "    # N = number of samples\n",
    "    # D = number of features\n",
    "    # labels is an np.array of size N times 1\n",
    "    def train(self, data, labels):\n",
    "        if self.depth == 0:\n",
    "            self.node = (self.most_freq(labels), 0, None, None)\n",
    "        else:\n",
    "            if self.features.any():\n",
    "                splited_data = get_binary_split(data, labels, self.features)\n",
    "            else:\n",
    "                splited_data = get_binary_split(data, labels, np.arange(len(data[0])))                \n",
    "            \n",
    "            #print(len(splited_data['left']))\n",
    "            #print(len(splited_data['right']))\n",
    "            \n",
    "            if (len(splited_data['left']) == 0) or (len(splited_data['right']) ==0):\n",
    "                self.depth = 0\n",
    "                self.node = (self.most_freq(labels), 0, None, None)\n",
    "            else:\n",
    "                self.node = (None, splited_data['gini'], splited_data['index'], splited_data['value'])\n",
    "                self.left = DecisionTree(self.depth-1)\n",
    "                self.left.train(data[splited_data['left']], labels[splited_data['left']])\n",
    "                \n",
    "                self.right = DecisionTree(self.depth-1)\n",
    "                self.right.train(data[splited_data['right']], labels[splited_data['right']])  \n",
    "    \n",
    "    # predict_single: self, np.array --> int\n",
    "    # row is a np.array of size D\n",
    "    def predict_single(self, row):\n",
    "        if self.depth == 0:\n",
    "            return int(self.node[0])\n",
    "        else:\n",
    "            index = self.node[2]\n",
    "            value = self.node[-1]\n",
    "\n",
    "            if row[index] < value:\n",
    "                return self.left.predict_single(row)\n",
    "            else:\n",
    "                return self.right.predict_single(row)\n",
    "                    \n",
    "    def predict(self, X):\n",
    "        l = X.shape[0]\n",
    "        ypred = np.empty(l, dtype=int)\n",
    "        \n",
    "        for i in range(0,l):\n",
    "            ypred[i] = self.predict_single(X[i])\n",
    "        \n",
    "        return ypred\n",
    "    \n",
    "    def show_structure(self, max_depth=-1):\n",
    "        if max_depth == -1:\n",
    "            max_depth = self.depth\n",
    "            \n",
    "        if self.depth > 0:\n",
    "            tabs = '\\t' * (max_depth-self.depth)\n",
    "            m = '{}[Depth {}: index={}, value={}, gini={}]'.format(tabs,\n",
    "                                                                   max_depth-self.depth+1, \n",
    "                                                                   self.node[2],\n",
    "                                                                   self.node[3],\n",
    "                                                                   round(self.node[1],3))\n",
    "            print(m)\n",
    "            if self.left != None:\n",
    "                self.left.show_structure(max_depth)\n",
    "            if self.right != None:\n",
    "                self.right.show_structure(max_depth)       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RForest(object):\n",
    "    def __init__(self, depth=1, num_bags=1, bag_size_ratio=0.6):\n",
    "        self.depth = depth\n",
    "        self.num_bags = num_bags\n",
    "        self.bag_size_ratio = bag_size_ratio\n",
    "        self.trees = []\n",
    "        \n",
    "    # train: self, np.array, np.array --> void\n",
    "    def train(self, data, labels):\n",
    "        n = data.shape[0]\n",
    "        nprime = int(self.bag_size_ratio * n)\n",
    "        \n",
    "        for i in range(0, self.num_bags):\n",
    "            index = np.random.randint(n, size=nprime)\n",
    "            sample_data = data[index]\n",
    "            sample_label = labels[index]\n",
    "            \n",
    "            features = np.random.randint(len(data[0]), size=np.random.randint(len(data[0])))\n",
    "            \n",
    "            T = DecisionTree(depth=self.depth, features=features)\n",
    "            T.train(sample_data, sample_label)\n",
    "            \n",
    "            self.trees.append(T)\n",
    "    \n",
    "    # predict: self, np.array --> np.array\n",
    "    def predict(self, X):\n",
    "        l = X.shape[0]\n",
    "        ypred = np.empty(l, dtype=int)\n",
    "        x = []\n",
    "        \n",
    "        for T in self.trees:\n",
    "            x.append(T.predict(X).reshape(l,1))\n",
    "        \n",
    "        results = np.concatenate(x, axis=0)\n",
    "        \n",
    "        #print(results)\n",
    "        \n",
    "        for i in range(0,l):\n",
    "            # find the most frequent occurence\n",
    "            counts = np.bincount(results[i])\n",
    "            ypred[i] = np.argmax(counts)\n",
    "            \n",
    "        \n",
    "        \n",
    "        return ypred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictions\n",
    "## 3.1 Banknotes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# data is first downloweded into DATA_PATH from \n",
    "# http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\n",
    "import os\n",
    "\n",
    "DATA_PATH = 'banknote'\n",
    "FILE_NAME = 'data_banknote_authentication.txt'\n",
    "\n",
    "def load_data(data_path=DATA_PATH, file_name=FILE_NAME):\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    return data.values[:, :-1], data.values[:,-1]\n",
    "\n",
    "DATA, LABELS = load_data()\n",
    "LABELS = LABELS * 1.0\n",
    "\n",
    "train_set = DATA[:1234, :]\n",
    "train_labels = LABELS[:1234]\n",
    "\n",
    "test_set = DATA[1234:,:]\n",
    "test_labels = LABELS[1234:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_auth = RForest(depth=4, num_bags=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_auth.train(train_set, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0072992700729927005"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = bank_auth.predict(test_set)\n",
    "e = len(test_labels[test_labels != ypred])/len(test_labels)\n",
    "e # error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous bench mark is: 0.0364963503649635. Give slight improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def prep_train_test(rate=0.9):\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    ind = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        ind.append(np.random.choice(50, int(50*rate), replace=False) + 50*i)\n",
    "    \n",
    "    train_ind = np.concatenate(ind)\n",
    "    test_ind = np.setdiff1d(np.arange(150), train_ind)\n",
    "        \n",
    "    \n",
    "    return X[train_ind], y[train_ind], X[test_ind], y[test_ind]\n",
    "\n",
    "iris_train_data, iris_train_labels, iris_test_data,iris_test_labels = prep_train_test(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clf = RForest(depth=4, num_bags=5)\n",
    "iris_clf.train(iris_train_data, iris_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = iris_clf.predict(iris_test_data)\n",
    "e = len(iris_test_labels[iris_test_labels != ypred])/len(iris_test_labels)\n",
    "e # error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "previous bench mark is: 0.06666666666666667. probably just Bayes error at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
