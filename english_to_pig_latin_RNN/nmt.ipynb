{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nmt.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TjPTaRB4mpCd","colab_type":"text"},"source":["# Colab FAQ\n","\n","For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n","\n","You need to use the colab GPU for this assignmentby selecting:\n","\n","> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"]},{"cell_type":"markdown","metadata":{"id":"s9IS9B9-yUU5","colab_type":"text"},"source":["## Setup PyTorch\n","All files are stored at /content/csc421/a3/ folder\n"]},{"cell_type":"code","metadata":{"id":"Z-6MQhMOlHXD","colab_type":"code","outputId":"ed2d36a0-eebc-4090-9151-b4fecb86c02c","executionInfo":{"status":"ok","timestamp":1575833141872,"user_tz":300,"elapsed":15705,"user":{"displayName":"Li Chen","photoUrl":"","userId":"08452733754503056217"}},"colab":{"base_uri":"https://localhost:8080/","height":661}},"source":["######################################################################\n","# Setup python environment and change the current working directory\n","######################################################################\n","!pip install torch torchvision\n","!pip install Pillow==4.0.0\n","%mkdir -p /content/csc421/a3/\n","%cd /content/csc421/a3"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (1.3.1+cu100)\n","Requirement already satisfied: torchvision in /usr/local/lib/python2.7/dist-packages (0.4.2+cu100)\n","Requirement already satisfied: future in /usr/local/lib/python2.7/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torch) (1.16.4)\n","Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.12.0)\n","Collecting pillow>=4.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/08/ff620ef5a6128ee6e7a505f5716f81fce7b71f3a69e99646ebe64e0b9984/Pillow-6.2.1-cp27-cp27mu-manylinux1_x86_64.whl (2.1MB)\n","\r\u001b[K     |▏                               | 10kB 27.9MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 37.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 43.3MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 48.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 41.7MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 44.1MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 33.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 35.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 92kB 37.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 34.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 34.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 122kB 34.4MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 34.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 143kB 34.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153kB 34.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 163kB 34.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 174kB 34.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 184kB 34.4MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 34.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 204kB 34.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 215kB 34.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 225kB 34.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 235kB 34.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 245kB 34.4MB/s eta 0:00:01\r\u001b[K     |████                            | 256kB 34.4MB/s eta 0:00:01\r\u001b[K     |████                            | 266kB 34.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 276kB 34.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 286kB 34.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 296kB 34.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 307kB 34.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 317kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 327kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 337kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 348kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 358kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 368kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 378kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 389kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 399kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 409kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 419kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 430kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 440kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 450kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 460kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 471kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 481kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 491kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 501kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 512kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 522kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 532kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 542kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 552kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 563kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 573kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 583kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 593kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 604kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 614kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 624kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 634kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 645kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 655kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 665kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 675kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 686kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 696kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 706kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 716kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 727kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 737kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 747kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 757kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 768kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 778kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 788kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 798kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 808kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 819kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 829kB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 839kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 849kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 860kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 870kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 880kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 890kB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 901kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 911kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 921kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 931kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 942kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 952kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 962kB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 972kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 983kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 993kB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.2MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.2MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.2MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.2MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.2MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.2MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.3MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.3MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.3MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.3MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.4MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.4MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.4MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.4MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.4MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.5MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.5MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.5MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.5MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.5MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.5MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.5MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.5MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.5MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.6MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.6MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.6MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.6MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.6MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.6MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.6MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.6MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.7MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.7MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.7MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.7MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.7MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.7MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.8MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.8MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.8MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.8MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.8MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.8MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.8MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.8MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.8MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.9MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.9MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.9MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.9MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.9MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.9MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.9MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.9MB 34.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.0MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 34.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 34.4MB/s \n","\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.3.1+cu100 which is incompatible.\u001b[0m\n","\u001b[?25hInstalling collected packages: pillow\n","  Found existing installation: Pillow 4.0.0\n","    Uninstalling Pillow-4.0.0:\n","      Successfully uninstalled Pillow-4.0.0\n","Successfully installed pillow-6.2.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting Pillow==4.0.0\n","  Using cached https://files.pythonhosted.org/packages/89/99/0e3522a9764fe371bf9f7729404b1ef7d9c4fc49cbe5f1761c6e07812345/Pillow-4.0.0-cp27-cp27mu-manylinux1_x86_64.whl\n","Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow==4.0.0) (0.46)\n","\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.3.1+cu100 which is incompatible.\u001b[0m\n","\u001b[31mERROR: scikit-image 0.14.3 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchvision 0.4.2+cu100 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n","Installing collected packages: Pillow\n","  Found existing installation: Pillow 6.2.1\n","    Uninstalling Pillow-6.2.1:\n","      Successfully uninstalled Pillow-6.2.1\n","Successfully installed Pillow-4.0.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["/content/csc421/a3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9DaTdRNuUra7","colab_type":"text"},"source":["# Helper code"]},{"cell_type":"markdown","metadata":{"id":"4BIpGwANoQOg","colab_type":"text"},"source":["## Utility functions"]},{"cell_type":"code","metadata":{"id":"D-UJHBYZkh7f","colab_type":"code","colab":{}},"source":["import os\n","import pdb\n","import argparse\n","import pickle as pkl\n","\n","from collections import defaultdict\n","\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch.autograd import Variable\n","\n","from six.moves.urllib.request import urlretrieve\n","import tarfile\n","import pickle\n","import sys\n","\n","\n","def get_file(fname,\n","             origin,\n","             untar=False,\n","             extract=False,\n","             archive_format='auto',\n","             cache_dir='data'):\n","    datadir = os.path.join(cache_dir)\n","    if not os.path.exists(datadir):\n","        os.makedirs(datadir)\n","\n","    if untar:\n","        untar_fpath = os.path.join(datadir, fname)\n","        fpath = untar_fpath + '.tar.gz'\n","    else:\n","        fpath = os.path.join(datadir, fname)\n","    \n","    print(fpath)\n","    if not os.path.exists(fpath):\n","        print('Downloading data from', origin)\n","\n","        error_msg = 'URL fetch failure on {}: {} -- {}'\n","        try:\n","            try:\n","                urlretrieve(origin, fpath)\n","            except URLError as e:\n","                raise Exception(error_msg.format(origin, e.errno, e.reason))\n","            except HTTPError as e:\n","                raise Exception(error_msg.format(origin, e.code, e.msg))\n","        except (Exception, KeyboardInterrupt) as e:\n","            if os.path.exists(fpath):\n","                os.remove(fpath)\n","            raise\n","\n","    if untar:\n","        if not os.path.exists(untar_fpath):\n","            print('Extracting file.')\n","            with tarfile.open(fpath) as archive:\n","                archive.extractall(datadir)\n","        return untar_fpath\n","\n","    if extract:\n","        _extract_archive(fpath, datadir, archive_format)\n","\n","    return fpath\n","\n","class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","        \n","def to_var(tensor, cuda):\n","    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n","\n","        Arguments:\n","            tensor: A Tensor object.\n","            cuda: A boolean flag indicating whether to use the GPU.\n","\n","        Returns:\n","            A Variable object, on the GPU if cuda==True.\n","    \"\"\"\n","    if cuda:\n","        return Variable(tensor.cuda())\n","    else:\n","        return Variable(tensor)\n","\n","\n","def create_dir_if_not_exists(directory):\n","    \"\"\"Creates a directory if it doesn't already exist.\n","    \"\"\"\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","def save_loss_plot(train_losses, val_losses, opts):\n","    \"\"\"Saves a plot of the training and validation loss curves.\n","    \"\"\"\n","    plt.figure()\n","    plt.plot(range(len(train_losses)), train_losses)\n","    plt.plot(range(len(val_losses)), val_losses)\n","    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n","    plt.xlabel('Epochs', fontsize=16)\n","    plt.ylabel('Loss', fontsize=16)\n","    plt.xticks(fontsize=14)\n","    plt.yticks(fontsize=14)\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n","    plt.close()\n","\n","\n","def checkpoint(encoder, decoder, idx_dict, opts):\n","    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n","    contains the char_to_index and index_to_char mappings, and the start_token\n","    and end_token values.\n","    \"\"\"\n","    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n","        torch.save(encoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n","        torch.save(decoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n","        pkl.dump(idx_dict, f)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbvpn4MaV0I1","colab_type":"text"},"source":["## Data loader"]},{"cell_type":"code","metadata":{"id":"XVT4TNTOV3Eg","colab_type":"code","colab":{}},"source":["def read_lines(filename):\n","    \"\"\"Read a file and split it into lines.\n","    \"\"\"\n","    lines = open(filename).read().strip().lower().split('\\n')\n","    return lines\n","\n","\n","def read_pairs(filename):\n","    \"\"\"Reads lines that consist of two words, separated by a space.\n","\n","    Returns:\n","        source_words: A list of the first word in each line of the file.\n","        target_words: A list of the second word in each line of the file.\n","    \"\"\"\n","    lines = read_lines(filename)\n","    source_words, target_words = [], []\n","    for line in lines:\n","        line = line.strip()\n","        if line:\n","            source, target = line.split()\n","            source_words.append(source)\n","            target_words.append(target)\n","    return source_words, target_words\n","\n","\n","def all_alpha_or_dash(s):\n","    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n","    \"\"\"\n","    return all(c.isalpha() or c == '-' for c in s)\n","\n","\n","def filter_lines(lines):\n","    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n","    \"\"\"\n","    return [line for line in lines if all_alpha_or_dash(line)]\n","\n","\n","def load_data():\n","    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n","    \"\"\"\n","\n","    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n","\n","    # Filter lines\n","    source_lines = filter_lines(source_lines)\n","    target_lines = filter_lines(target_lines)\n","\n","    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n","\n","    # Create a dictionary mapping each character to a unique index\n","    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n","\n","    # Add start and end tokens to the dictionary\n","    start_token = len(char_to_index)\n","    end_token = len(char_to_index) + 1\n","    char_to_index['SOS'] = start_token\n","    char_to_index['EOS'] = end_token\n","\n","    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n","    index_to_char = { index: char for (char, index) in char_to_index.items() }\n","\n","    # Store the final size of the vocabulary\n","    vocab_size = len(char_to_index)\n","\n","    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n","\n","    idx_dict = { 'char_to_index': char_to_index,\n","                 'index_to_char': index_to_char,\n","                 'start_token': start_token,\n","                 'end_token': end_token }\n","\n","    return line_pairs, vocab_size, idx_dict\n","\n","\n","def create_dict(pairs):\n","    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n","    This is used to make batches: each batch consists of two parallel tensors, one containing\n","    all source indexes and the other containing all corresponding target indexes.\n","    Within a batch, all the source words are the same length, and all the target words are\n","    the same length.\n","    \"\"\"\n","    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n","\n","    d = defaultdict(list)\n","    for (s,t) in unique_pairs:\n","        d[(len(s), len(t))].append((s,t))\n","\n","    return d\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bRWfRdmVVjUl","colab_type":"text"},"source":["## Training and evaluation code"]},{"cell_type":"code","metadata":{"id":"wa5-onJhoSeM","colab_type":"code","colab":{}},"source":["def string_to_index_list(s, char_to_index, end_token):\n","    \"\"\"Converts a sentence into a list of indexes (for each character).\n","    \"\"\"\n","    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n","\n","\n","def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n","    words (whitespace-separated), running the encoder-decoder model to translate each\n","    word independently, and then stitching the words back together with spaces between them.\n","    \"\"\"\n","    if idx_dict is None:\n","      line_pairs, vocab_size, idx_dict = load_data()\n","    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n","\n","\n","def translate(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a given string from English to Pig-Latin.\n","    \"\"\"\n","\n","    char_to_index = idx_dict['char_to_index']\n","    index_to_char = idx_dict['index_to_char']\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","\n","    max_generated_chars = 20\n","    gen_string = ''\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_last_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_last_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    for i in range(max_generated_chars):\n","      ## slow decoding, recompute everything at each time\n","      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n","      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","      ni = ni[-1] #latest output token\n","\n","      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","      \n","      if ni == end_token:\n","          break\n","      else:\n","          gen_string = \"\".join(\n","              [index_to_char[int(item)] \n","               for item in generated_words.cpu().numpy().reshape(-1)])\n","\n","    return gen_string\n","\n","\n","def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n","    \"\"\"\n","    if idx_dict is None:\n","      line_pairs, vocab_size, idx_dict = load_data()\n","    char_to_index = idx_dict['char_to_index']\n","    index_to_char = idx_dict['index_to_char']\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","\n","    max_generated_chars = 20\n","    gen_string = ''\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    produced_end_token = False\n","\n","    for i in range(max_generated_chars):\n","      ## slow decoding, recompute everything at each time\n","      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n","      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","      ni = ni[-1] #latest output token\n","      \n","      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","      \n","      if ni == end_token:\n","          break\n","      else:\n","          gen_string = \"\".join(\n","              [index_to_char[int(item)] \n","               for item in generated_words.cpu().numpy().reshape(-1)])\n","    \n","    if isinstance(attention_weights, tuple):\n","      ## transformer's attention mweights\n","      attention_weights, self_attention_weights = attention_weights\n","    \n","    all_attention_weights = attention_weights.data.cpu().numpy()\n","    \n","    for i in range(len(all_attention_weights)):\n","      attention_weights_matrix = all_attention_weights[i].squeeze()\n","      fig = plt.figure()\n","      ax = fig.add_subplot(111)\n","      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n","      fig.colorbar(cax)\n","\n","      # Set up axes\n","      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n","      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n","\n","      # Show label at every tick\n","      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","      # Add title\n","      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n","      plt.tight_layout()\n","      plt.grid('off')\n","      plt.show()\n","      #plt.savefig(save)\n","\n","      #plt.close(fig)\n","\n","    return gen_string\n","\n","\n","def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n","    \"\"\"Train/Evaluate the model on a dataset.\n","\n","    Arguments:\n","        data_dict: The validation/test word pairs, organized by source and target lengths.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n","        opts: The command-line arguments.\n","\n","    Returns:\n","        mean_loss: The average loss over all batches from data_dict.\n","    \"\"\"\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","    char_to_index = idx_dict['char_to_index']\n","\n","    losses = []\n","    for key in data_dict:\n","        input_strings, target_strings = zip(*data_dict[key])\n","        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n","        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n","\n","        num_tensors = len(input_tensors)\n","        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n","\n","        for i in range(num_batches):\n","\n","            start = i * opts.batch_size\n","            end = start + opts.batch_size\n","\n","            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n","            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n","\n","            # The batch size may be different in each epoch\n","            BS = inputs.size(0)\n","\n","            encoder_annotations, encoder_hidden = encoder(inputs)\n","\n","            # The last hidden state of the encoder becomes the first hidden state of the decoder\n","            decoder_hidden = encoder_hidden\n","\n","            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n","            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n","\n","            loss = 0.0\n","\n","            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n","\n","            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n","            \n","            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n","            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n","            targets_flatten = targets.view(-1)\n","            loss = criterion(decoder_outputs_flatten, targets_flatten)\n","\n","            losses.append(loss.item())\n","\n","            ## training if an optimizer is provided\n","            if optimizer:\n","              # Zero gradients\n","              optimizer.zero_grad()\n","              # Compute gradients\n","              loss.backward()\n","              # Update the parameters of the encoder and decoder\n","              optimizer.step()\n","              \n","    mean_loss = np.mean(losses)\n","    return mean_loss\n","\n","  \n","\n","def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n","    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n","        * Prints training and val loss each epoch.\n","        * Prints qualitative translation results each epoch using TEST_SENTENCE\n","        * Saves an attention map for TEST_WORD_ATTN each epoch\n","\n","    Arguments:\n","        train_dict: The training word pairs, organized by source and target lengths.\n","        val_dict: The validation word pairs, organized by source and target lengths.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n","        opts: The command-line arguments.\n","    \"\"\"\n","\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","    char_to_index = idx_dict['char_to_index']\n","\n","    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n","\n","    best_val_loss = 1e6\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(opts.nepochs):\n","\n","        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n","        \n","        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n","        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n","\n","        if val_loss < best_val_loss:\n","            checkpoint(encoder, decoder, idx_dict, opts)\n","\n","        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n","        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n","\n","        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n","        loss_log.flush()\n","\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","\n","        save_loss_plot(train_losses, val_losses, opts)\n","\n","\n","def print_data_stats(line_pairs, vocab_size, idx_dict):\n","    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n","    \"\"\"\n","    print('=' * 80)\n","    print('Data Stats'.center(80))\n","    print('-' * 80)\n","    for pair in line_pairs[:5]:\n","        print(pair)\n","    print('Num unique word pairs: {}'.format(len(line_pairs)))\n","    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n","    print('Vocab size: {}'.format(vocab_size))\n","    print('=' * 80)\n","\n","\n","def train(opts):\n","    line_pairs, vocab_size, idx_dict = load_data()\n","    print_data_stats(line_pairs, vocab_size, idx_dict)\n","\n","    # Split the line pairs into an 80% train and 20% val split\n","    num_lines = len(line_pairs)\n","    num_train = int(0.8 * num_lines)\n","    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n","\n","    # Group the data by the lengths of the source and target words, to form batches\n","    train_dict = create_dict(train_pairs)\n","    val_dict = create_dict(val_pairs)\n","\n","    ##########################################################################\n","    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n","    ##########################################################################\n","    encoder = GRUEncoder(vocab_size=vocab_size, \n","                         hidden_size=opts.hidden_size, \n","                         opts=opts)\n","\n","    if opts.decoder_type == 'rnn':\n","        decoder = RNNDecoder(vocab_size=vocab_size, \n","                             hidden_size=opts.hidden_size)\n","    elif opts.decoder_type == 'rnn_attention':\n","        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n","                                      hidden_size=opts.hidden_size, \n","                                      attention_type=opts.attention_type)\n","    elif opts.decoder_type == 'transformer':\n","        decoder = TransformerDecoder(vocab_size=vocab_size, \n","                                     hidden_size=opts.hidden_size, \n","                                     num_layers=opts.num_transformer_layers)\n","    else:\n","        raise NotImplementedError\n","        \n","    #### setup checkpoint path\n","    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n","                                      opts.batch_size, \n","                                      opts.decoder_type)\n","    opts.checkpoint_path = model_name\n","    create_dir_if_not_exists(opts.checkpoint_path)\n","    ####\n","\n","    if opts.cuda:\n","        encoder.cuda()\n","        decoder.cuda()\n","        print(\"Moved models to GPU!\")\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n","\n","    try:\n","        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n","    except KeyboardInterrupt:\n","        print('Exiting early from training.')\n","        return encoder, decoder\n","      \n","    return encoder, decoder\n","\n","\n","def print_opts(opts):\n","    \"\"\"Prints the values of all command-line arguments.\n","    \"\"\"\n","    print('=' * 80)\n","    print('Opts'.center(80))\n","    print('-' * 80)\n","    for key in opts.__dict__:\n","        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n","    print('=' * 80)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bXNsLNkOn38w","colab_type":"text"},"source":["# Your code for NMT models"]},{"cell_type":"markdown","metadata":{"id":"_BAfi_8yWB3y","colab_type":"text"},"source":["## GRU cell"]},{"cell_type":"code","metadata":{"id":"9ztmyA5Ro67o","colab_type":"code","colab":{}},"source":["class MyGRUCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(MyGRUCell, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        vari = 1.0/np.sqrt(input_size)\n","        varh = 1.0/np.sqrt(hidden_size)\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        ## Input linear layers\n","        self.Wiz = torch.Tensor(hidden_size, input_size).cuda().uniform_(-vari, vari)\n","        self.Wir = torch.Tensor(hidden_size, input_size).cuda().uniform_(-vari, vari)\n","        self.Wih = torch.Tensor(hidden_size, input_size).cuda().uniform_(-vari, vari)\n","        #torch.empty(self.hidden_size, self.input_size).uniform_(-vari, vari).cuda()\n","\n","        ## Hidden linear layers\n","        self.Whz = torch.Tensor(hidden_size, hidden_size).cuda().uniform_(-varh, varh)\n","        self.Whr = torch.Tensor(hidden_size, hidden_size).cuda().uniform_(-varh, varh)\n","        self.Whh = torch.Tensor(hidden_size, hidden_size).cuda().uniform_(-varh, varh)\n","        #torch.empty(self.hidden_size, self.hidden_size).uniform_(-varh, varh).cuda()\n","        #print(self.Whh.dtype)\n","\n","        ## bias\n","        self.br = torch.Tensor(hidden_size).cuda().uniform_(-varh, varh)\n","        self.bz = torch.Tensor(hidden_size).cuda().uniform_(-varh, varh)\n","        self.bg = torch.Tensor(hidden_size).cuda().uniform_(-varh, varh)\n","        #torch.empty(self.hidden_size).uniform_(-varh, varh).cuda()\n","        #print(self.br.dtype)\n","        \n","    def sig(self, x):\n","      return 1/(1 + torch.exp(-x))\n","\n","    def forward(self, x, h_prev):\n","        \"\"\"Forward pass of the GRU computation for one time step.\n","\n","        Arguments\n","            x: batch_size x input_size\n","            h_prev: batch_size x hidden_size\n","\n","        Returns:\n","            h_new: batch_size x hidden_size\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        z = self.sig(x.mm(self.Wiz.t()) + h_prev.mm(self.Whz.t()) + self.bz)\n","        r = self.sig(x.mm(self.Wir.t()) + h_prev.mm(self.Whr.t()) + self.br)\n","        g = torch.tanh(x.mm(self.Wih.t()) + h_prev.mm(self.Whh.t()) + self.bg)\n","        h_new = (1-z)*g + z*h_prev\n","        return h_new\n","\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-JBVFLEZWNC1","colab_type":"text"},"source":["### GRU encoder / decoder"]},{"cell_type":"code","metadata":{"id":"xaDt7XDmWRzC","colab_type":"code","colab":{}},"source":["class GRUEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, opts):\n","        super(GRUEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.gru = nn.GRUCell(hidden_size, hidden_size)\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        hidden = self.init_hidden(batch_size)\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","        annotations = []\n","\n","        for i in range(seq_len):\n","            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n","            hidden = self.gru(x, hidden)\n","            annotations.append(hidden)\n","\n","        annotations = torch.stack(annotations, dim=1)\n","        return annotations, hidden\n","\n","    def init_hidden(self, bs):\n","        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n","        of a batch of sequences.\n","\n","        Arguments:\n","            bs: The batch size for the initial hidden state.\n","\n","        Returns:\n","            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n","        \"\"\"\n","        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n","\n","\n","class RNNDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(RNNDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the non-attentional decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch. (batch_size x seq_len)\n","            annotations: This is not used here. It just maintains consistency with the\n","                    interface used by the AttentionDecoder class.\n","            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            None\n","        \"\"\"        \n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n","\n","        hiddens = []\n","        h_prev = hidden_init\n","        for i in range(seq_len):\n","            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n","            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n","            hiddens.append(h_prev)\n","\n","        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n","        \n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, None      \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tWe0RO5FWajD","colab_type":"text"},"source":["## Attention"]},{"cell_type":"code","metadata":{"id":"9GUK5A7CWhV8","colab_type":"code","colab":{}},"source":["class AdditiveAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(AdditiveAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        # A two layer fully-connected network\n","        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n","        self.attention_network = nn.Sequential(\n","                                    nn.Linear(hidden_size*2, hidden_size),\n","                                    nn.ReLU(),\n","                                    nn.Linear(hidden_size, 1)\n","                                 )\n","\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the additive attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state. (batch_size x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x 1 x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The attention_weights must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size, seq_len, _ = keys.shape\n","        #print(batch_size, queries.size())\n","        expanded_queries = queries.unsqueeze(dim=1).expand_as(keys) # batch_size x 1 x hidden_size\n","        concat_inputs = torch.cat((expanded_queries,keys), dim=2) # batch_size x seq_len x (2*hidden_size)\n","        unnormalized_attention = self.attention_network(concat_inputs) # batch_size x seq_len x hidden_size\n","        attention_weights = self.softmax(unnormalized_attention) # batch_size x seq_len x 1\n","        context = torch.bmm(attention_weights.view(batch_size, 1, seq_len), values) # batch_size x 1 x hidden_size\n","        return context, attention_weights\n","        \n","      \n","\n","class ScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(ScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size = queries.shape[0]\n","        q = self.Q(queries)\n","        if len(q.shape) < 3:\n","          q = q.unsqueeze(dim=1)  # q = batch x k x hidden_size\n","        #print(q.shape)\n","        \n","        k = self.K(keys) # batch x seq_len x hidden_size\n","        #print(k.shape)\n","        v = self.V(values) # batch x seq_len x hidden_size\n","        \n","        unnormalized_attention = torch.bmm(k, torch.transpose(q, 1, 2))/self.scaling_factor # batch x seq_len x k\n","        attention_weights = self.softmax(unnormalized_attention) # batch x seq_len x k\n","        context = torch.bmm(torch.transpose(attention_weights, 1, 2), v) # batch x k x hidden_size\n","        return context, attention_weights\n","        \n","\n","      \n","      \n","class CausalScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(CausalScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.neg_inf = torch.tensor(-1e7)\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) seq_len x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x seq_len x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size = queries.shape[0]\n","        q = self.Q(queries) # batch x k (seq_len) x hidden_size\n","        k = self.K(keys) # batch x seq_len x hidden_size\n","        #print(k.shape)\n","        v = self.V(values) # batch x seq_len x hidden_size\n","        \n","\n","        unnormalized_attention = torch.bmm(k, torch.transpose(q, 1, 2))/self.scaling_factor # batch x seq_len x k (seq_len)\n","        mask = (torch.tril(torch.ones(unnormalized_attention.size()), -1) * self.neg_inf).cuda()\n","        attention_weights = self.softmax(unnormalized_attention + mask) # batch x seq_len x k\n","        context = torch.bmm(torch.transpose(attention_weights, 1, 2), v) # batch x k (seq_len) x hidden_size\n","        return context, attention_weights\n","        \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pemjZo2XWtRt","colab_type":"text"},"source":["### Attention decoder"]},{"cell_type":"code","metadata":{"id":"PfjF0Z-PWwPv","colab_type":"code","colab":{}},"source":["class RNNAttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n","        super(RNNAttentionDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n","        if attention_type == 'additive':\n","          self.attention = AdditiveAttention(hidden_size=hidden_size)\n","        elif attention_type == 'scaled_dot':\n","          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n","        \n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        \n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        where\n","            context: weighted average of the values (batch_size x 1 x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","        \"\"\"\n","        \n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x decoder_seq_len x hidden_size        \n","\n","        hiddens = []\n","        attentions = []\n","        h_prev = hidden_init\n","        #print(h_prev.size(), 'asdfsdfasdfasdf')\n","        for i in range(seq_len):\n","            # ------------\n","            # FILL THIS IN\n","            # ------------\n","            embed_current = embed[:,i,:] # batch_size x hidden_size\n","            # print(h_prev.size())\n","            context, attention_weights = self.attention.forward(h_prev, annotations, annotations)\n","            #print(context.squeeze().shape)\n","            #print(embed_current.shape)\n","            embed_and_context = torch.cat((context.squeeze(dim=1), embed_current), dim=1)\n","            h_prev = self.rnn(embed_and_context, h_prev)\n","\n","            \n","            hiddens.append(h_prev)\n","            attentions.append(attention_weights)\n","\n","        hiddens = torch.stack(hiddens, dim=1) # batch_size x decoder_seq_len x hidden_size\n","        attentions = torch.cat(attentions, dim=2) # batch_size x decoder_seq_len x seq_len\n","        \n","        output = self.out(hiddens)  # batch_size x decoder_seq_len x vocab_size\n","        return output, attentions\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N8JpcwTRW5cw","colab_type":"text"},"source":["### Transformer decoder"]},{"cell_type":"code","metadata":{"id":"V5vJPku1W7sz","colab_type":"code","colab":{}},"source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers):\n","        super(TransformerDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n","        self.num_layers = num_layers\n","        \n","        self.self_attentions = nn.ModuleList([CausalScaledDotAttention( #ScaledDotAttention( \n","                                    hidden_size=hidden_size, \n","                                 ) for i in range(self.num_layers)])\n","        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n","                                    hidden_size=hidden_size, \n","                                 ) for i in range(self.num_layers)])\n","        self.attention_mlps = nn.ModuleList([nn.Sequential(\n","                                    nn.Linear(hidden_size, hidden_size),\n","                                    nn.ReLU(),\n","                                 ) for i in range(self.num_layers)])\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        \n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: Not used in the transformer decoder\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","        \n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n","\n","        encoder_attention_weights_list = []\n","        self_attention_weights_list = []\n","        contexts = embed\n","        for i in range(self.num_layers):\n","          # ------------\n","          # FILL THIS IN\n","          # ------------\n","          new_contexts, self_attention_weights = self.self_attentions[i](contexts, annotations, annotations)\n","          residual_contexts = contexts + new_contexts\n","          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations)\n","          residual_contexts = residual_contexts + new_contexts\n","          new_contexts = residual_contexts + F.relu(residual_contexts)\n","          contexts = self.attention_mlps[i](new_contexts)\n","\n","          \n","          encoder_attention_weights_list.append(encoder_attention_weights)\n","          self_attention_weights_list.append(self_attention_weights)\n","          \n","        output = self.out(contexts)\n","        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n","        self_attention_weights = torch.stack(self_attention_weights_list)\n","        \n","        return output, (encoder_attention_weights, self_attention_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XuNFd6LNo0-o","colab_type":"text"},"source":["# Training\n"]},{"cell_type":"markdown","metadata":{"id":"kiUwiOITHTW4","colab_type":"text"},"source":["## Download dataset"]},{"cell_type":"code","metadata":{"id":"xwcFjsEpHRbI","colab_type":"code","outputId":"1ec46dac-88bc-43e2-a725-709972920f9d","executionInfo":{"status":"ok","timestamp":1575833143451,"user_tz":300,"elapsed":17203,"user":{"displayName":"Li Chen","photoUrl":"","userId":"08452733754503056217"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["######################################################################\n","# Download Translation datasets\n","######################################################################\n","data_fpath = get_file(fname='pig_latin_data.txt', \n","                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n","                         untar=False)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["data/pig_latin_data.txt\n","('Downloading data from', 'http://www.cs.toronto.edu/~jba/pig_latin_data.txt')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hmQmyJDSRFKR","colab_type":"text"},"source":["## RNN decoder"]},{"cell_type":"code","metadata":{"id":"0LKaRF1jwhH7","colab_type":"code","outputId":"de753af8-5176-443a-d3cf-6af3bf88843b","executionInfo":{"status":"ok","timestamp":1575660945253,"user_tz":300,"elapsed":239795,"user":{"displayName":"Li Chen","photoUrl":"","userId":"08452733754503056217"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n","              'attention_type': '',  # options: additive / scaled_dot\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","rnn_encoder, rnn_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: rnn                                    \n","                               lr_decay: 0.99                                   \n","                         attention_type:                                        \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RNNDecoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:   0 | Train loss: 2.428 | Val loss: 2.019 | Gen: esessay ay esessay ay esessay\n","Epoch:   1 | Train loss: 1.948 | Val loss: 1.853 | Gen: esay ay ontessay estay onestay\n","Epoch:   2 | Train loss: 1.784 | Val loss: 1.747 | Gen: erestay angedway ontessay ingsay ongestay\n","Epoch:   3 | Train loss: 1.664 | Val loss: 1.667 | Gen: eray aredway onthedway ingstay onthedway\n","Epoch:   4 | Train loss: 1.572 | Val loss: 1.613 | Gen: eray aredway onghthay isthay onghestay\n","Epoch:   5 | Train loss: 1.507 | Val loss: 1.569 | Gen: eray aredway onghtay istay onghthay\n","Epoch:   6 | Train loss: 1.444 | Val loss: 1.527 | Gen: esay aredway onghtiongay istay onghthay\n","Epoch:   7 | Train loss: 1.391 | Val loss: 1.504 | Gen: estay aringway ongighthay issay onghingtay\n","Epoch:   8 | Train loss: 1.346 | Val loss: 1.467 | Gen: estay aringway onthingsay issay onghthay\n","Epoch:   9 | Train loss: 1.311 | Val loss: 1.458 | Gen: ethay aringway onithingway issway onghthay\n","Epoch:  10 | Train loss: 1.268 | Val loss: 1.397 | Gen: ethay aringway onithingway issway onglyway\n","Epoch:  11 | Train loss: 1.236 | Val loss: 1.393 | Gen: ececay aringway oningthay issay ollyway\n","Epoch:  12 | Train loss: 1.224 | Val loss: 1.364 | Gen: entay aringway oninglysay issway oledway\n","Epoch:  13 | Train loss: 1.186 | Val loss: 1.325 | Gen: ethay aringway oningthay issway ollyway\n","Epoch:  14 | Train loss: 1.151 | Val loss: 1.318 | Gen: ethay aridedway oningthay issway ollyway\n","Epoch:  15 | Train loss: 1.133 | Val loss: 1.294 | Gen: entay ariday oninglysay issway oldgay\n","Epoch:  16 | Train loss: 1.107 | Val loss: 1.260 | Gen: ethay ariday oninglyfay isway oldtay\n","Epoch:  17 | Train loss: 1.074 | Val loss: 1.245 | Gen: eytay aridedway oninglytay issway oldthay\n","Epoch:  18 | Train loss: 1.057 | Val loss: 1.242 | Gen: ethay arway oninglytay issway ollway\n","Epoch:  19 | Train loss: 1.054 | Val loss: 1.254 | Gen: ethay ariday onginglay issway oldgray\n","Epoch:  20 | Train loss: 1.036 | Val loss: 1.220 | Gen: etay ariday ongingtray isway oldedway\n","Epoch:  21 | Train loss: 1.018 | Val loss: 1.213 | Gen: eytay aridedway oninglypay issay oldgray\n","Epoch:  22 | Train loss: 0.995 | Val loss: 1.191 | Gen: ehay aridedway oninglypay isway oldednay\n","Epoch:  23 | Train loss: 0.987 | Val loss: 1.193 | Gen: etay aridedway oninglypay issway oldgray\n","Epoch:  24 | Train loss: 0.974 | Val loss: 1.171 | Gen: ehay ariday onginglay isway olndedway\n","Epoch:  25 | Train loss: 0.960 | Val loss: 1.152 | Gen: ehay ariday oninglecay issay oldednay\n","Epoch:  26 | Train loss: 0.942 | Val loss: 1.156 | Gen: ehtay aridedway oninglecay isway olndedway\n","Epoch:  27 | Train loss: 0.952 | Val loss: 1.146 | Gen: ehay ariday oninglestay isway oldednay\n","Epoch:  28 | Train loss: 0.918 | Val loss: 1.139 | Gen: ehay aridedway oninglecay isway oldengray\n","Epoch:  29 | Train loss: 0.908 | Val loss: 1.111 | Gen: ehay aridedway oninglestay isway oldengray\n","Epoch:  30 | Train loss: 0.886 | Val loss: 1.099 | Gen: ehay arirway oninglecay isway oldengray\n","Epoch:  31 | Train loss: 0.881 | Val loss: 1.106 | Gen: ehay aridedway oninglestay isway oldednay\n","Epoch:  32 | Train loss: 0.868 | Val loss: 1.093 | Gen: ehay arirway oninglestay isway oldednay\n","Epoch:  33 | Train loss: 0.870 | Val loss: 1.090 | Gen: ehay ariday oninglecay isway oldengray\n","Epoch:  34 | Train loss: 0.884 | Val loss: 1.103 | Gen: ehay aridedway oningletay-inway-awl isway oldednay\n","Epoch:  35 | Train loss: 0.857 | Val loss: 1.067 | Gen: ehay ariday oninglytay isway oldingway\n","Epoch:  36 | Train loss: 0.849 | Val loss: 1.065 | Gen: ehay ariday oningletay isway olndedway\n","Epoch:  37 | Train loss: 0.839 | Val loss: 1.063 | Gen: ehay ariday oninglytay isway oldednay\n","Epoch:  38 | Train loss: 0.835 | Val loss: 1.060 | Gen: ehay ariday onginglestay isway oldengray\n","Epoch:  39 | Train loss: 0.812 | Val loss: 1.041 | Gen: ehay ariday oninglytay isway oldengray\n","Epoch:  40 | Train loss: 0.797 | Val loss: 1.032 | Gen: ehay ariday oningletay isway oldengray\n","Epoch:  41 | Train loss: 0.798 | Val loss: 1.034 | Gen: ehay ariday ondingtioncay isway oldengray\n","Epoch:  42 | Train loss: 0.787 | Val loss: 1.041 | Gen: ehay ariday ondingtioncay isway oldengray\n","Epoch:  43 | Train loss: 0.779 | Val loss: 1.037 | Gen: ehay ariday ondingtioncay isway oldengray\n","Epoch:  44 | Train loss: 0.780 | Val loss: 1.039 | Gen: ehay aidray ondingtioncay isway oldengay\n","Epoch:  45 | Train loss: 0.807 | Val loss: 1.075 | Gen: etay ariday oninglestay-ondnay isway oldfay\n","Epoch:  46 | Train loss: 0.781 | Val loss: 1.052 | Gen: ehay ariday ondingtioncay isway oldengray\n","Epoch:  47 | Train loss: 0.762 | Val loss: 1.008 | Gen: ehay ariday ondingtioncay isway olndypay\n","Epoch:  48 | Train loss: 0.752 | Val loss: 1.037 | Gen: ehay ariday ondingtioncay isway oldengray\n","Epoch:  49 | Train loss: 0.752 | Val loss: 1.012 | Gen: ehay aidray onglinistedway isway olndycay\n","Epoch:  50 | Train loss: 0.744 | Val loss: 1.018 | Gen: ehay aidray onglingstay isway oldengray\n","Epoch:  51 | Train loss: 0.733 | Val loss: 1.012 | Gen: ehay aidray ondingtioncay isway oldengray\n","Epoch:  52 | Train loss: 0.729 | Val loss: 1.004 | Gen: ehay aidray ondingtioncay isway oldengay\n","Epoch:  53 | Train loss: 0.725 | Val loss: 1.004 | Gen: ehay aidray ondingtinecay isway oldengray\n","Epoch:  54 | Train loss: 0.717 | Val loss: 0.994 | Gen: ehay aidray ondingtioncay isway oldengray\n","Epoch:  55 | Train loss: 0.711 | Val loss: 0.997 | Gen: ehay aidray onglinistyway isway oldengray\n","Epoch:  56 | Train loss: 0.708 | Val loss: 1.016 | Gen: ehay aidray ondingtingsay isway olndedcay\n","Epoch:  57 | Train loss: 0.708 | Val loss: 0.993 | Gen: ehay aidray ondingtingshay isway odlenay\n","Epoch:  58 | Train loss: 0.715 | Val loss: 0.999 | Gen: ehay aidray onglinistedway isway odlenay\n","Epoch:  59 | Train loss: 0.702 | Val loss: 0.973 | Gen: etay airway ondingtinecay isway olndedcay\n","Epoch:  60 | Train loss: 0.700 | Val loss: 0.984 | Gen: ehay aidray ondingtingsay isway odlenay\n","Epoch:  61 | Train loss: 0.700 | Val loss: 0.980 | Gen: ehay airway ondingtingsay isway olndedcay\n","Epoch:  62 | Train loss: 0.697 | Val loss: 0.976 | Gen: ehay aidray ondingtingsray isway odlenay\n","Epoch:  63 | Train loss: 0.684 | Val loss: 0.960 | Gen: ethay airway onglinistysay isway odlenay\n","Epoch:  64 | Train loss: 0.668 | Val loss: 0.965 | Gen: ehay airway ondingtingsray isway odlenay\n","Epoch:  65 | Train loss: 0.663 | Val loss: 0.977 | Gen: ehay airway ondingtingsray isway odlenay\n","Epoch:  66 | Train loss: 0.658 | Val loss: 0.949 | Gen: ehay airway ondingtingsray isway odlenay\n","Epoch:  67 | Train loss: 0.676 | Val loss: 1.002 | Gen: ehay airway ondingtingsay isway odlenay\n","Epoch:  68 | Train loss: 0.696 | Val loss: 0.965 | Gen: ethay airbay onglinistysay isway olndedcay\n","Epoch:  69 | Train loss: 0.690 | Val loss: 0.947 | Gen: ehay airway onglinitionsway isway odlengay\n","Epoch:  70 | Train loss: 0.656 | Val loss: 0.953 | Gen: ehatingay airway ondingtinecay isway odlenay\n","Epoch:  71 | Train loss: 0.646 | Val loss: 0.941 | Gen: ehay airway ondingtingsray isway odlenay\n","Epoch:  72 | Train loss: 0.641 | Val loss: 0.943 | Gen: etay airway onglinitypay isway odlengay\n","Epoch:  73 | Train loss: 0.632 | Val loss: 0.915 | Gen: ehay airway ondingtinecay isway odlengay\n","Epoch:  74 | Train loss: 0.626 | Val loss: 0.935 | Gen: etay airway ondingtingsray isway odlengay\n","Epoch:  75 | Train loss: 0.624 | Val loss: 0.916 | Gen: etay airway ondingtinecay isway odlengay\n","Epoch:  76 | Train loss: 0.625 | Val loss: 0.931 | Gen: etay airway ondingtingsray isway odlengay\n","Epoch:  77 | Train loss: 0.620 | Val loss: 0.917 | Gen: etay airway ondingtinecay isway odlengay\n","Epoch:  78 | Train loss: 0.620 | Val loss: 0.920 | Gen: etay airway ondingtinecay isway odlengay\n","Epoch:  79 | Train loss: 0.643 | Val loss: 0.919 | Gen: etay airway ondingtingsay isway odlengay\n","Epoch:  80 | Train loss: 0.630 | Val loss: 0.961 | Gen: ehay airway ondingtinecay isway odlengay\n","Epoch:  81 | Train loss: 0.623 | Val loss: 0.904 | Gen: etay airway ondingtingsray isway odlengay-oungsay\n","Epoch:  82 | Train loss: 0.617 | Val loss: 0.920 | Gen: ethay airway onglinistednay isway odlengray\n","Epoch:  83 | Train loss: 0.606 | Val loss: 0.920 | Gen: etay airway ondingtinecyway isway odlengray\n","Epoch:  84 | Train loss: 0.602 | Val loss: 0.900 | Gen: etay airway ondingtingsray isway odlengray\n","Epoch:  85 | Train loss: 0.596 | Val loss: 0.895 | Gen: etay airway ondingtinecay isway odlengray\n","Epoch:  86 | Train loss: 0.590 | Val loss: 0.907 | Gen: etay airway ondingtinecay isway odlengray\n","Epoch:  87 | Train loss: 0.595 | Val loss: 0.904 | Gen: etay airway ondingtinecay isway odlengray\n","Epoch:  88 | Train loss: 0.590 | Val loss: 0.912 | Gen: etay airway ondingtinecay isway odlengray\n","Epoch:  89 | Train loss: 0.599 | Val loss: 0.920 | Gen: etay airway ondingtinecay isway odlengray\n","Epoch:  90 | Train loss: 0.595 | Val loss: 0.906 | Gen: etay airway ondingtinecay isway odlengray\n","Epoch:  91 | Train loss: 0.583 | Val loss: 0.901 | Gen: etay airway ondingtinecay isway odlengray\n","Epoch:  92 | Train loss: 0.583 | Val loss: 0.891 | Gen: etay airway ondingtinecay isway odlencygay\n","Epoch:  93 | Train loss: 0.584 | Val loss: 0.919 | Gen: etay airway ondingtinecay isway odlencay\n","Epoch:  94 | Train loss: 0.584 | Val loss: 0.888 | Gen: etay airway ondingtinecay isway odlencay\n","Epoch:  95 | Train loss: 0.570 | Val loss: 0.885 | Gen: etay airway ondingtinecay isway odlencay\n","Epoch:  96 | Train loss: 0.575 | Val loss: 0.884 | Gen: etay airway ondingtingsray isway odlencygay\n","Epoch:  97 | Train loss: 0.571 | Val loss: 0.893 | Gen: etay airway ondingtinecay isway odlencygay\n","Epoch:  98 | Train loss: 0.566 | Val loss: 0.910 | Gen: etay airway ondingtinecay isway odlencay\n","Epoch:  99 | Train loss: 0.570 | Val loss: 0.883 | Gen: etay airway onglinistedcay isway odlencygay\n","source:\t\tthe air conditioning is working \n","translated:\tetay airway onglinistedcay isway odlencygay\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p2kPGj5DFv7a","colab_type":"code","outputId":"f48392b2-27f8-4c31-f7f1-a804c7c6ae75","executionInfo":{"status":"ok","timestamp":1575661399411,"user_tz":300,"elapsed":978,"user":{"displayName":"Li Chen","photoUrl":"","userId":"08452733754503056217"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["TEST_SENTENCE = 'the air conditioning is working'\n","translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["source:\t\tthe air conditioning is working \n","translated:\tetay airway onglinistedcay isway odlencygay\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7cP7nl5NRJbu","colab_type":"text"},"source":["## RNN attention decoder"]},{"cell_type":"code","metadata":{"id":"nKlyfbuPDXDR","colab_type":"code","outputId":"09ff03da-b229-4a48-cc05-7079e2398892","executionInfo":{"status":"ok","timestamp":1575660705210,"user_tz":300,"elapsed":623213,"user":{"displayName":"Li Chen","photoUrl":"","userId":"08452733754503056217"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n","              'attention_type': 'additive',  # options: additive / scaled_dot\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","rnn_attn_encoder, rnn_attn_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: rnn_attention                          \n","                               lr_decay: 0.99                                   \n","                         attention_type: additive                               \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRUCell. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RNNAttentionDecoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MyGRUCell. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AdditiveAttention. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:   0 | Train loss: 2.728 | Val loss: 2.397 | Gen: eeay eay eay eay oneray\n","Epoch:   1 | Train loss: 2.214 | Val loss: 2.047 | Gen: eseray ay onesay ay onteray\n","Epoch:   2 | Train loss: 1.963 | Val loss: 1.899 | Gen: eseday aray ongsay ay ongsay\n","Epoch:   3 | Train loss: 1.832 | Val loss: 1.792 | Gen: estay aray ongay ateray ongthay\n","Epoch:   4 | Train loss: 1.737 | Val loss: 1.713 | Gen: estay aray ontingay osthay onghentay\n","Epoch:   5 | Train loss: 1.652 | Val loss: 1.634 | Gen: edtay araray ontingay ostay onthengay\n","Epoch:   6 | Train loss: 1.586 | Val loss: 1.576 | Gen: eday aray ontingay ostray ounghenday\n","Epoch:   7 | Train loss: 1.514 | Val loss: 1.535 | Gen: etthay araray ontingay oshay ounghengay\n","Epoch:   8 | Train loss: 1.461 | Val loss: 1.467 | Gen: edtay araray ontingay oshay oringheday\n","Epoch:   9 | Train loss: 1.399 | Val loss: 1.410 | Gen: eday araray ontingay oshay oringhay\n","Epoch:  10 | Train loss: 1.341 | Val loss: 1.358 | Gen: eday araray ontintingay issay orighengay\n","Epoch:  11 | Train loss: 1.285 | Val loss: 1.339 | Gen: ethay araray ondingay isshay orighentay\n","Epoch:  12 | Train loss: 1.242 | Val loss: 1.274 | Gen: eday ariray onditingingay isshay orighentay\n","Epoch:  13 | Train loss: 1.180 | Val loss: 1.233 | Gen: ethay ariray onditingingay asistay oritingay\n","Epoch:  14 | Train loss: 1.143 | Val loss: 1.196 | Gen: eday ariray onditingingay usay orighengay\n","Epoch:  15 | Train loss: 1.089 | Val loss: 1.131 | Gen: eway ariray ondtiongingay usay orocingay\n","Epoch:  16 | Train loss: 1.073 | Val loss: 1.208 | Gen: ethay array onditingay usay oritingay\n","Epoch:  17 | Train loss: 1.034 | Val loss: 1.055 | Gen: eway airay onditingay-ingay-ion usay orotugghay\n","Epoch:  18 | Train loss: 0.983 | Val loss: 1.064 | Gen: ethay ariray onditioningay astray orotinghay\n","Epoch:  19 | Train loss: 0.952 | Val loss: 0.989 | Gen: eway airay onditiongay usay orkingay\n","Epoch:  20 | Train loss: 0.894 | Val loss: 0.960 | Gen: eway airay onditiongay usay orkinghay\n","Epoch:  21 | Train loss: 0.875 | Val loss: 0.928 | Gen: eway airay onditionitingay astray orkingay\n","Epoch:  22 | Train loss: 0.861 | Val loss: 0.899 | Gen: eway airay onditioningay atray orkinghay\n","Epoch:  23 | Train loss: 0.830 | Val loss: 0.874 | Gen: ethay airay onditiongay-ingay-io isway orkinggay\n","Epoch:  24 | Train loss: 0.890 | Val loss: 1.009 | Gen: eway array onditiongay ispay orkingay\n","Epoch:  25 | Train loss: 0.905 | Val loss: 0.858 | Gen: ethay airay onditiongay istay ay-ighngay\n","Epoch:  26 | Train loss: 0.785 | Val loss: 0.817 | Gen: ethay airay onditiongay ispay arkingglay\n","Epoch:  27 | Train loss: 0.739 | Val loss: 0.795 | Gen: ethay airay onditioningay ispay orkingway\n","Epoch:  28 | Train loss: 0.738 | Val loss: 0.862 | Gen: ethay airay onditioninistingay isway orkingway\n","Epoch:  29 | Train loss: 0.741 | Val loss: 0.760 | Gen: ethay airay onditioningay isway orkinggay\n","Epoch:  30 | Train loss: 0.688 | Val loss: 0.734 | Gen: ethay airay onditioningay isway orkinggay\n","Epoch:  31 | Train loss: 0.683 | Val loss: 0.743 | Gen: ethay airay onditioningay isway orkingway\n","Epoch:  32 | Train loss: 0.639 | Val loss: 0.725 | Gen: ethay airay onditioningay ispay orkingway\n","Epoch:  33 | Train loss: 0.634 | Val loss: 0.703 | Gen: ethay airay onditionintay issay orkingway\n","Epoch:  34 | Train loss: 0.608 | Val loss: 0.682 | Gen: ethay airay onditioningway issay orkinggay\n","Epoch:  35 | Train loss: 0.599 | Val loss: 0.709 | Gen: eway airay onditioningay isway orkingway\n","Epoch:  36 | Train loss: 0.598 | Val loss: 0.680 | Gen: ethay airay onditiongistay isway orkingway\n","Epoch:  37 | Train loss: 0.583 | Val loss: 0.672 | Gen: ethay airay onditioningay isway orkingway\n","Epoch:  38 | Train loss: 0.573 | Val loss: 0.682 | Gen: ethay airay onditioningay isway orkinggay\n","Epoch:  39 | Train loss: 0.568 | Val loss: 0.651 | Gen: ethay airay onditioningay isway orkingway\n","Epoch:  40 | Train loss: 0.547 | Val loss: 0.638 | Gen: ethay airay onditioningay isway orkingway\n","Epoch:  41 | Train loss: 0.523 | Val loss: 0.628 | Gen: ethay airay onditioningay isway orkinggay\n","Epoch:  42 | Train loss: 0.517 | Val loss: 0.611 | Gen: ethay airay onditioningay isway orkingsay\n","Epoch:  43 | Train loss: 0.547 | Val loss: 0.665 | Gen: ehtay airay onditionintay isway arkinggay\n","Epoch:  44 | Train loss: 0.543 | Val loss: 0.608 | Gen: ethay airay onditiongway isway orkingway\n","Epoch:  45 | Train loss: 0.508 | Val loss: 0.579 | Gen: ethay airay onditioningway isway orkinggay\n","Epoch:  46 | Train loss: 0.480 | Val loss: 0.589 | Gen: ehtay airay onditioningay isway orkingway\n","Epoch:  47 | Train loss: 0.486 | Val loss: 0.602 | Gen: ehtay airay onditioningway isway orkinggray\n","Epoch:  48 | Train loss: 0.458 | Val loss: 0.597 | Gen: ehtay airay onditionintay issay orkingway\n","Epoch:  49 | Train loss: 0.546 | Val loss: 0.635 | Gen: ehtay airay onditionintay isway orkingway\n","Epoch:  50 | Train loss: 0.519 | Val loss: 0.616 | Gen: eway airay onditionintay ispay orkingway\n","Epoch:  51 | Train loss: 0.476 | Val loss: 0.567 | Gen: ehtay airay onditioningay issay orkinggray\n","Epoch:  52 | Train loss: 0.434 | Val loss: 0.543 | Gen: ehay airay onditioningay issay orkingway\n","Epoch:  53 | Train loss: 0.415 | Val loss: 0.536 | Gen: ehay airay onditioningway issay orkingway\n","Epoch:  54 | Train loss: 0.408 | Val loss: 0.544 | Gen: ehay airay onditioningway issay orkingway\n","Epoch:  55 | Train loss: 0.419 | Val loss: 0.525 | Gen: ehay airay onditioningay issay orkingway\n","Epoch:  56 | Train loss: 0.397 | Val loss: 0.531 | Gen: ehay airay onditioningway issay orkingway\n","Epoch:  57 | Train loss: 0.385 | Val loss: 0.514 | Gen: ehay airay onditioningay issay orkingway\n","Epoch:  58 | Train loss: 0.389 | Val loss: 0.525 | Gen: ehay airay onditioningay issay orkingway\n","Epoch:  59 | Train loss: 0.379 | Val loss: 0.496 | Gen: ehay airay onditionintay issay orkingway\n","Epoch:  60 | Train loss: 0.386 | Val loss: 0.575 | Gen: ehay airay onditioningway issay orkingway\n","Epoch:  61 | Train loss: 0.509 | Val loss: 0.533 | Gen: ehay airay onditioningay ispay orkingway\n","Epoch:  62 | Train loss: 0.406 | Val loss: 0.535 | Gen: ehay airray onditionintay issay orkingway\n","Epoch:  63 | Train loss: 0.374 | Val loss: 0.511 | Gen: ehay airay onditionintay issay orkingway\n","Epoch:  64 | Train loss: 0.351 | Val loss: 0.505 | Gen: ehay airay onditioningay issay orkingsay\n","Epoch:  65 | Train loss: 0.345 | Val loss: 0.494 | Gen: ehay airay onditioningay issay orkingsay\n","Epoch:  66 | Train loss: 0.337 | Val loss: 0.487 | Gen: ehay airay onditionintay issay orkingsay\n","Epoch:  67 | Train loss: 0.332 | Val loss: 0.494 | Gen: ehay airay onditioningay issay orkingsay\n","Epoch:  68 | Train loss: 0.342 | Val loss: 0.501 | Gen: ehay airday onditioningay ispay orkingsay\n","Epoch:  69 | Train loss: 0.357 | Val loss: 0.492 | Gen: ehay airay onditionintay issay orkingsay\n","Epoch:  70 | Train loss: 0.360 | Val loss: 0.515 | Gen: ehay airday onditionintay issay orkingsay\n","Epoch:  71 | Train loss: 0.431 | Val loss: 0.483 | Gen: ehay airay onditionintay issay orkingway\n","Epoch:  72 | Train loss: 0.400 | Val loss: 0.513 | Gen: ehay airday onditioninsay issay orkingsay\n","Epoch:  73 | Train loss: 0.346 | Val loss: 0.482 | Gen: ehay airday onditionintay issay orkingway\n","Epoch:  74 | Train loss: 0.324 | Val loss: 0.486 | Gen: ehay airday onditionintay issay orkingsay\n","Epoch:  75 | Train loss: 0.313 | Val loss: 0.455 | Gen: ehay airday onditionintay issay orkingsay\n","Epoch:  76 | Train loss: 0.300 | Val loss: 0.456 | Gen: ehay airday onditionintay issay orkingsay\n","Epoch:  77 | Train loss: 0.295 | Val loss: 0.448 | Gen: ehay airday onditionintay issay orkingsay\n","Epoch:  78 | Train loss: 0.289 | Val loss: 0.451 | Gen: ehay airday onditioningay issay orkingsay\n","Epoch:  79 | Train loss: 0.286 | Val loss: 0.447 | Gen: ehay airday onditioningay issay orkingsay\n","Epoch:  80 | Train loss: 0.282 | Val loss: 0.458 | Gen: ehay airday onditioningay issay orkingsay\n","Epoch:  81 | Train loss: 0.283 | Val loss: 0.436 | Gen: ehay airday onditionintay issay orkingsay\n","Epoch:  82 | Train loss: 0.357 | Val loss: 0.562 | Gen: ehay airday onditioninsay issay orkingsay\n","Epoch:  83 | Train loss: 0.366 | Val loss: 0.433 | Gen: ehay airday onditioningay isway orkingsay\n","Epoch:  84 | Train loss: 0.307 | Val loss: 0.442 | Gen: ehay airday onditioninway issay orkingsay\n","Epoch:  85 | Train loss: 0.282 | Val loss: 0.413 | Gen: ehay airday onditioningay issay orkingsay\n","Epoch:  86 | Train loss: 0.271 | Val loss: 0.412 | Gen: ehay airday onditioningay issay orkingsay\n","Epoch:  87 | Train loss: 0.265 | Val loss: 0.406 | Gen: ehay airday onditioningay issay orkingsay\n","Epoch:  88 | Train loss: 0.261 | Val loss: 0.419 | Gen: ehay airday onditioningay issay orkingsay\n","Epoch:  89 | Train loss: 0.259 | Val loss: 0.392 | Gen: ehay airday onditioningay issay orkingsay\n","Epoch:  90 | Train loss: 0.307 | Val loss: 0.432 | Gen: ehay airday onditioninway ispay orkingsay\n","Epoch:  91 | Train loss: 0.383 | Val loss: 0.396 | Gen: ehay airway onditioninway ispay orkingsay\n","Epoch:  92 | Train loss: 0.280 | Val loss: 0.384 | Gen: ehay airday onditioninway ispray orkingsay\n","Epoch:  93 | Train loss: 0.263 | Val loss: 0.382 | Gen: ehay airday onditioningay ispray orkingsay\n","Epoch:  94 | Train loss: 0.253 | Val loss: 0.381 | Gen: ehay airday onditioningay issay orkingsay\n","Epoch:  95 | Train loss: 0.246 | Val loss: 0.374 | Gen: ehay airday onditioningay issay orkingsay\n","Epoch:  96 | Train loss: 0.242 | Val loss: 0.371 | Gen: ehay airway onditioningay issay orkingsay\n","Epoch:  97 | Train loss: 0.238 | Val loss: 0.373 | Gen: ehay airway onditioningay issay orkingsay\n","Epoch:  98 | Train loss: 0.236 | Val loss: 0.372 | Gen: ehay airway onditioningay issay orkingsay\n","Epoch:  99 | Train loss: 0.233 | Val loss: 0.374 | Gen: ehay airway onditioningway issay orkingsay\n","source:\t\tthe air conditioning is working \n","translated:\tehay airway onditioningway issay orkingsay\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vE-hKCxhF3iR","colab_type":"code","outputId":"4f17949d-3559-4242-fee7-d13af444f919","executionInfo":{"status":"ok","timestamp":1575661414227,"user_tz":300,"elapsed":823,"user":{"displayName":"Li Chen","photoUrl":"","userId":"08452733754503056217"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["TEST_SENTENCE = 'the air conditioning is working'\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["source:\t\tthe air conditioning is working \n","translated:\tehay airway onditioningway issay orkingsay\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k4_noH8M3cIw","colab_type":"code","outputId":"bcae6708-1c37-46e9-e7e8-5754d8d7b52d","executionInfo":{"status":"ok","timestamp":1575664414207,"user_tz":300,"elapsed":658065,"user":{"displayName":"Li Chen","photoUrl":"","userId":"08452733754503056217"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n","              'attention_type': 'scaled_dot',  # options: additive / scaled_dot\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","rnn_attn_encoder, rnn_attn_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: rnn_attention                          \n","                               lr_decay: 0.99                                   \n","                         attention_type: scaled_dot                             \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ScaledDotAttention. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:   0 | Train loss: 2.727 | Val loss: 2.408 | Gen: eeeay eay eeeeeeay eeay eeeeay\n","Epoch:   1 | Train loss: 2.259 | Val loss: 2.135 | Gen: eereay ay eereray eeresay eeeresay\n","Epoch:   2 | Train loss: 2.047 | Val loss: 2.007 | Gen: eereday ay oonreday inssesay insssesesesesesesese\n","Epoch:   3 | Train loss: 1.918 | Val loss: 1.863 | Gen: eteray ay oonstay insay insesededay\n","Epoch:   4 | Train loss: 1.797 | Val loss: 1.789 | Gen: ereday ay onongsay insay onsesededay\n","Epoch:   5 | Train loss: 1.722 | Val loss: 1.713 | Gen: eteray eray ongstingstay insay ongssay\n","Epoch:   6 | Train loss: 1.672 | Val loss: 1.793 | Gen: elay eray oongay inray instay\n","Epoch:   7 | Train loss: 1.644 | Val loss: 1.621 | Gen: eteday aray ontongay inay onghay\n","Epoch:   8 | Train loss: 1.546 | Val loss: 1.563 | Gen: eteday array ongrgay insay ontedgay\n","Epoch:   9 | Train loss: 1.601 | Val loss: 1.565 | Gen: etay array ontghay insay ortinghay\n","Epoch:  10 | Train loss: 1.487 | Val loss: 1.501 | Gen: etesay aray onghay insay ortinghay\n","Epoch:  11 | Train loss: 1.417 | Val loss: 1.470 | Gen: etay array ongrgay-ongay issay ortinghay\n","Epoch:  12 | Train loss: 1.398 | Val loss: 1.506 | Gen: etay ay ongsay issay ortinghay\n","Epoch:  13 | Train loss: 1.401 | Val loss: 1.394 | Gen: eteday airay ongray-ongway issay ortinghay\n","Epoch:  14 | Train loss: 1.293 | Val loss: 1.324 | Gen: eteday airay ongrgay-ongway isay ortinghay\n","Epoch:  15 | Train loss: 1.239 | Val loss: 1.325 | Gen: eteray airay ondgangray-ongway isay orktingway\n","Epoch:  16 | Train loss: 1.348 | Val loss: 1.358 | Gen: etay airay onddgay-ongway isay orksgay\n","Epoch:  17 | Train loss: 1.251 | Val loss: 1.286 | Gen: etay airay ondgay-ongay isay orkongay\n","Epoch:  18 | Train loss: 1.211 | Val loss: 1.301 | Gen: etray airray ongdgay isay orkngway\n","Epoch:  19 | Train loss: 1.153 | Val loss: 1.203 | Gen: etray airay ondgangray-ongway isay orkongway\n","Epoch:  20 | Train loss: 1.091 | Val loss: 1.179 | Gen: etray airay ondgangray-ongway isay orkongway\n","Epoch:  21 | Train loss: 1.067 | Val loss: 1.112 | Gen: etray airay ondgay-ongway isay orrghingay\n","Epoch:  22 | Train loss: 1.032 | Val loss: 1.085 | Gen: etray airay ondgangway-ongway isay orkingway\n","Epoch:  23 | Train loss: 0.981 | Val loss: 1.112 | Gen: etray airay ondingdngway isay orkongway\n","Epoch:  24 | Train loss: 0.965 | Val loss: 1.037 | Gen: etray airay ondtay-ongway isay orkongway\n","Epoch:  25 | Train loss: 0.955 | Val loss: 1.049 | Gen: etray airay ondtiongway-ongway isay orkinghay\n","Epoch:  26 | Train loss: 0.894 | Val loss: 1.071 | Gen: etray airay ondtingway isay orkinghay\n","Epoch:  27 | Train loss: 0.873 | Val loss: 0.962 | Gen: etray airay onday-orngway isay orkinghay\n","Epoch:  28 | Train loss: 0.910 | Val loss: 1.119 | Gen: etay airay onday-ongway isay orkengway\n","Epoch:  29 | Train loss: 0.890 | Val loss: 0.941 | Gen: etay airay onday-orngway isay orkingway\n","Epoch:  30 | Train loss: 0.813 | Val loss: 0.952 | Gen: etray airay ondationgingay issay orkinghay\n","Epoch:  31 | Train loss: 0.766 | Val loss: 0.879 | Gen: etray airay onday-orongway issay orkinghay\n","Epoch:  32 | Train loss: 0.776 | Val loss: 0.854 | Gen: etray airay onday-orongway issay orkinghay\n","Epoch:  33 | Train loss: 0.705 | Val loss: 0.857 | Gen: etray airay onditiongway issay orkinghay\n","Epoch:  34 | Train loss: 0.689 | Val loss: 0.790 | Gen: etray airay onditiongway issay orkinghay\n","Epoch:  35 | Train loss: 0.665 | Val loss: 0.882 | Gen: etray airway onditiongingay issway orkinghay\n","Epoch:  36 | Train loss: 0.705 | Val loss: 0.856 | Gen: etray airinay onditioningway issway orkinghay\n","Epoch:  37 | Train loss: 0.623 | Val loss: 0.756 | Gen: etray airway onditioningway issay orkinghay\n","Epoch:  38 | Train loss: 0.619 | Val loss: 0.793 | Gen: etray airway onditiongway issay orkingway\n","Epoch:  39 | Train loss: 0.670 | Val loss: 0.743 | Gen: etray airway onditioningnay isway orkinghay\n","Epoch:  40 | Train loss: 0.596 | Val loss: 0.817 | Gen: etray airway onditioncingnay isway orkinghay\n","Epoch:  41 | Train loss: 0.806 | Val loss: 0.747 | Gen: etway airway onditioningnday issay orkingsay\n","Epoch:  42 | Train loss: 0.643 | Val loss: 0.750 | Gen: etray airway onditioningday isway orkingray\n","Epoch:  43 | Train loss: 0.612 | Val loss: 0.792 | Gen: etray airway onditiongway issay orkingsay\n","Epoch:  44 | Train loss: 0.573 | Val loss: 0.629 | Gen: etway airway onditiongingay isway orkingray\n","Epoch:  45 | Train loss: 0.530 | Val loss: 0.647 | Gen: etway airway onditiongingay isway orkingray\n","Epoch:  46 | Train loss: 0.518 | Val loss: 0.625 | Gen: etray airway onditioningnay isway orkinghay\n","Epoch:  47 | Train loss: 0.537 | Val loss: 0.601 | Gen: etway airway onditioningnay issay orkingsay\n","Epoch:  48 | Train loss: 0.518 | Val loss: 0.629 | Gen: etray airway onditioningnay isway orkingsay\n","Epoch:  49 | Train loss: 0.533 | Val loss: 0.651 | Gen: etsay airway onditioningday issay orkingray\n","Epoch:  50 | Train loss: 0.502 | Val loss: 0.604 | Gen: etray airway onditioningnay isway orkingsay\n","Epoch:  51 | Train loss: 0.896 | Val loss: 0.873 | Gen: etway airway ondcednay issay orksingay\n","Epoch:  52 | Train loss: 0.673 | Val loss: 0.757 | Gen: etway airway onditiondnay issay orkingray\n","Epoch:  53 | Train loss: 0.580 | Val loss: 0.664 | Gen: etway airway onditsaingingway isway orkingsay\n","Epoch:  54 | Train loss: 0.541 | Val loss: 0.677 | Gen: etway airway onditioningnay issway orkingray\n","Epoch:  55 | Train loss: 0.531 | Val loss: 0.651 | Gen: etway airway onditioningay issway orkingsay\n","Epoch:  56 | Train loss: 0.489 | Val loss: 0.583 | Gen: etway airway onditioningnay isway orkingsay\n","Epoch:  57 | Train loss: 0.463 | Val loss: 0.565 | Gen: etway airway onditioningnay isway orkingray\n","Epoch:  58 | Train loss: 0.450 | Val loss: 0.577 | Gen: etway airway onditioningnay issay orkingray\n","Epoch:  59 | Train loss: 0.436 | Val loss: 0.563 | Gen: etray airway onditioningnay issay orkingray\n","Epoch:  60 | Train loss: 0.441 | Val loss: 0.582 | Gen: etway airway onditioningay isway orkingray\n","Epoch:  61 | Train loss: 0.476 | Val loss: 0.552 | Gen: etway airway onditiongnay isway orkingway\n","Epoch:  62 | Train loss: 0.451 | Val loss: 0.560 | Gen: etway airway onditioningnay isway orkingway\n","Epoch:  63 | Train loss: 0.444 | Val loss: 0.557 | Gen: etray airway onditioningnay isway orkingsay\n","Epoch:  64 | Train loss: 0.434 | Val loss: 0.515 | Gen: etray airway onditioningnay isway orkingway\n","Epoch:  65 | Train loss: 0.412 | Val loss: 0.526 | Gen: etway airway onditioningnay issay orkingway\n","Epoch:  66 | Train loss: 0.390 | Val loss: 0.486 | Gen: etray airway onditioningnay isway orkingway\n","Epoch:  67 | Train loss: 0.377 | Val loss: 0.508 | Gen: etray airway onditioningday isway orkingway\n","Epoch:  68 | Train loss: 0.382 | Val loss: 0.486 | Gen: ethay airway onditioningnay isway orkingway\n","Epoch:  69 | Train loss: 0.471 | Val loss: 0.527 | Gen: ethay airway onditioningnay isway orkingway\n","Epoch:  70 | Train loss: 0.416 | Val loss: 0.498 | Gen: ethay airway onditioningnay isway orkingway\n","Epoch:  71 | Train loss: 0.481 | Val loss: 0.804 | Gen: efray airway onditionway isstay orkingway\n","Epoch:  72 | Train loss: 0.552 | Val loss: 0.555 | Gen: efray airway onditioningday isway orkingway\n","Epoch:  73 | Train loss: 0.419 | Val loss: 0.502 | Gen: efay airway onditioningnay isway orkingway\n","Epoch:  74 | Train loss: 0.433 | Val loss: 0.640 | Gen: ethay airway onditioningnay issay orkingray\n","Epoch:  75 | Train loss: 0.432 | Val loss: 0.475 | Gen: efay airway onditioningnay isway orkingway\n","Epoch:  76 | Train loss: 0.393 | Val loss: 0.504 | Gen: efay airway onditioningnay isway orkingway\n","Epoch:  77 | Train loss: 0.356 | Val loss: 0.462 | Gen: efay airway onditioningnay isway orkingway\n","Epoch:  78 | Train loss: 0.336 | Val loss: 0.450 | Gen: ehhay airway onditioningnay isway orkingway\n","Epoch:  79 | Train loss: 0.350 | Val loss: 0.497 | Gen: efay airway onditioningnay isway orkingway\n","Epoch:  80 | Train loss: 0.366 | Val loss: 0.461 | Gen: ehhay airway onditioningnay isway orkingway\n","Epoch:  81 | Train loss: 0.328 | Val loss: 0.467 | Gen: ehhay airway onditioningcay isway orkingway\n","Epoch:  82 | Train loss: 0.324 | Val loss: 0.449 | Gen: efay airway onditioningnay isway orkingway\n","Epoch:  83 | Train loss: 0.429 | Val loss: 0.894 | Gen: ehhay airway onditiongcay issay orkingsay\n","Epoch:  84 | Train loss: 0.460 | Val loss: 0.470 | Gen: efay airway onditioningcay isway orkingway\n","Epoch:  85 | Train loss: 0.336 | Val loss: 0.445 | Gen: efay airway onditioningcay isway orkingway\n","Epoch:  86 | Train loss: 0.311 | Val loss: 0.429 | Gen: efay airway onditioningcay isway orkingway\n","Epoch:  87 | Train loss: 0.296 | Val loss: 0.432 | Gen: efay airway onditioningnay isway orkingway\n","Epoch:  88 | Train loss: 0.288 | Val loss: 0.443 | Gen: efay airway onditioningcay isway orkingway\n","Epoch:  89 | Train loss: 0.301 | Val loss: 0.423 | Gen: ehhay airway onditioningnay isway orkingway\n","Epoch:  90 | Train loss: 0.398 | Val loss: 0.490 | Gen: efay airway onditionway isway orkingway\n","Epoch:  91 | Train loss: 0.464 | Val loss: 0.476 | Gen: efay airway onditiongncay isway orkingway\n","Epoch:  92 | Train loss: 0.370 | Val loss: 0.428 | Gen: efay airway onditioningnay isway orkingway\n","Epoch:  93 | Train loss: 0.308 | Val loss: 0.421 | Gen: efhay airway onditioningncay isway orkingway\n","Epoch:  94 | Train loss: 0.302 | Val loss: 0.438 | Gen: efay airway onditioningnay isway orkingway\n","Epoch:  95 | Train loss: 0.300 | Val loss: 0.444 | Gen: efhay airway onditioningcay isway orkingway\n","Epoch:  96 | Train loss: 0.308 | Val loss: 0.425 | Gen: efay airway onditioningcay isway orkingway\n","Epoch:  97 | Train loss: 0.280 | Val loss: 0.407 | Gen: efay airway onditioningcay isway orkingway\n","Epoch:  98 | Train loss: 0.292 | Val loss: 0.425 | Gen: efay airway onditioningcay isway orkingway\n","Epoch:  99 | Train loss: 0.275 | Val loss: 0.383 | Gen: efay airway onditioningcay isway orkingway\n","source:\t\tthe air conditioning is working \n","translated:\tefay airway onditioningcay isway orkingway\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kfov3EN-7z5g","colab_type":"code","outputId":"2e1227c1-18f8-4633-df79-5a2afbf66784","executionInfo":{"status":"ok","timestamp":1575664736314,"user_tz":300,"elapsed":1047,"user":{"displayName":"Li Chen","photoUrl":"","userId":"08452733754503056217"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["TEST_SENTENCE = 'the math building is large'\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["source:\t\tthe math building is large \n","translated:\tefay athmay uildingway isway argeway\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X8FaZZUWRpY9","colab_type":"text"},"source":["## Transformer"]},{"cell_type":"code","metadata":{"id":"Ik5rx9qw9KCg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"47349b20-3ed2-4c06-b49f-5bff367dac66","executionInfo":{"status":"ok","timestamp":1575836232749,"user_tz":300,"elapsed":275954,"user":{"displayName":"Li Chen","photoUrl":"","userId":"08452733754503056217"}}},"source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n","              'num_transformer_layers': 3,\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","transformer_encoder, transformer_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                 num_transformer_layers: 3                                      \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: transformer                            \n","                               lr_decay: 0.99                                   \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.225 | Val loss: 1.882 | Gen: eretay annay onnrttttttttay enay onnnnnnay\n","Epoch:   1 | Train loss: 1.769 | Val loss: 1.704 | Gen: erey ayiy onnnnnnnnnnny iny orunnnny\n","Epoch:   2 | Train loss: 1.655 | Val loss: 1.606 | Gen: ereway ancay onngngngngngay inay ongtonnay\n","Epoch:   3 | Train loss: 1.569 | Val loss: 1.537 | Gen: ereway aypay ongngngngngnay inay ongengey\n","Epoch:   4 | Train loss: 1.490 | Val loss: 1.477 | Gen: ertay ailay ongngngngngnay ilay onglingway\n","Epoch:   5 | Train loss: 1.419 | Val loss: 1.428 | Gen: eteway ainay ongiongiongitay ilay oungtlitay\n","Epoch:   6 | Train loss: 1.359 | Val loss: 1.346 | Gen: eteway ainay iongciongcioway isay onghtrtay\n","Epoch:   7 | Train loss: 1.316 | Val loss: 1.312 | Gen: ethay aicay ongciongiongay ilay oungrinay\n","Epoch:   8 | Train loss: 1.292 | Val loss: 1.294 | Gen: ethay ainay ongciongcionay inway oringrray\n","Epoch:   9 | Train loss: 1.240 | Val loss: 1.343 | Gen: ethay airay ongcececececay isay ory-orhay\n","Epoch:  10 | Train loss: 1.248 | Val loss: 1.318 | Gen: ethay airay ongingingingay isway orlngry\n","Epoch:  11 | Train loss: 1.196 | Val loss: 1.206 | Gen: eheway airay ongiongiongiday isay ornghngway\n","Epoch:  12 | Train loss: 1.181 | Val loss: 1.323 | Gen: eheway airay ongcuncutionway isay orly\n","Epoch:  13 | Train loss: 1.437 | Val loss: 1.427 | Gen: eheway airay ongincecececay isay orherheway\n","Epoch:  14 | Train loss: 1.317 | Val loss: 1.313 | Gen: estay airay onctititititay isay orunglray\n","Epoch:  15 | Train loss: 1.222 | Val loss: 1.283 | Gen: ethay airay ongcecececonway isay orllrlray\n","Epoch:  16 | Train loss: 1.157 | Val loss: 1.182 | Gen: ethay airay ongcecececonway isay orwrwly\n","Epoch:  17 | Train loss: 1.230 | Val loss: 1.292 | Gen: ethay airay ongcingcingcay inay orhery-ay\n","Epoch:  18 | Train loss: 1.157 | Val loss: 1.248 | Gen: ethay airay ongcecingcecay isipay orhgyheway\n","Epoch:  19 | Train loss: 1.099 | Val loss: 1.188 | Gen: ethay airay ongcecececonay isay orhgrheway\n","Epoch:  20 | Train loss: 1.099 | Val loss: 1.263 | Gen: eshay auray ongcingcengcay isay orgmgmay\n","Epoch:  21 | Train loss: 1.062 | Val loss: 1.190 | Gen: ethay airay ongcingcingcay isay orhlywway\n","Epoch:  22 | Train loss: 1.113 | Val loss: 1.214 | Gen: ethay airay ongcececececay isay orhtingway\n","Epoch:  23 | Train loss: 1.056 | Val loss: 1.150 | Gen: eheway aumay ongcccccccceway isay orghingay\n","Epoch:  24 | Train loss: 1.018 | Val loss: 1.160 | Gen: eheway airay ongcgcgcgcgcay isay orlwingay\n","Epoch:  25 | Train loss: 1.002 | Val loss: 1.136 | Gen: eheway airay ongcititititay isay orglyblay\n","Epoch:  26 | Train loss: 0.974 | Val loss: 1.051 | Gen: ethay airay ongcingcingcay isay orsgyhinway\n","Epoch:  27 | Train loss: 0.933 | Val loss: 1.082 | Gen: ethay airay ongcingcingcay isay orwlywinay\n","Epoch:  28 | Train loss: 0.921 | Val loss: 1.246 | Gen: ethay airay aititiongcionay isway orwwswtay\n","Epoch:  29 | Train loss: 1.160 | Val loss: 1.197 | Gen: ettay airway ongcingcingcay isay orsgsgsay\n","Epoch:  30 | Train loss: 1.087 | Val loss: 1.167 | Gen: ethay airay ongcingcingcay isway orsinghay\n","Epoch:  31 | Train loss: 1.139 | Val loss: 1.163 | Gen: ethay ireway ongciciongceway isway owlinghay\n","Epoch:  32 | Train loss: 1.094 | Val loss: 1.269 | Gen: ely ireway ongciongcuenway isway owinghiray\n","Epoch:  33 | Train loss: 1.148 | Val loss: 1.199 | Gen: elhay aaiway ongcengcccutay isay owlingway\n","Epoch:  34 | Train loss: 1.083 | Val loss: 1.195 | Gen: ethay airway ongcingcengcay isay orwaygway\n","Epoch:  35 | Train loss: 1.076 | Val loss: 1.182 | Gen: ethay airway ongcengionginay isay orwwwweway\n","Epoch:  36 | Train loss: 1.043 | Val loss: 1.151 | Gen: eheway airay ongcitiongctay isay orwwwwiway\n","Epoch:  37 | Train loss: 1.012 | Val loss: 1.124 | Gen: ethay airay ongcccccingiway isway orwwwwway\n","Epoch:  38 | Train loss: 0.986 | Val loss: 1.139 | Gen: ethay airay onengcingcutay isay orwssssay\n","Epoch:  39 | Train loss: 0.978 | Val loss: 1.105 | Gen: ethay airway ongctcececkcay isay orwwwwway\n","Epoch:  40 | Train loss: 0.941 | Val loss: 1.079 | Gen: ethay airay ongcingcingcay isay orwwwwway\n","Epoch:  41 | Train loss: 0.947 | Val loss: 1.109 | Gen: eheway airway ongcingciongay isway orgwwloway\n","Epoch:  42 | Train loss: 1.020 | Val loss: 1.111 | Gen: ethay airay ongcungcuongay isway orwwwwway\n","Epoch:  43 | Train loss: 0.925 | Val loss: 1.056 | Gen: ethay airay ongindcececeway isay orwwwway\n","Epoch:  44 | Train loss: 0.883 | Val loss: 1.037 | Gen: ethay airay ongcecececonay isway orwwwway\n","Epoch:  45 | Train loss: 0.945 | Val loss: 1.170 | Gen: ethay airway ongctiiiiiiiiiiiiiii isway orwwthiway\n","Epoch:  46 | Train loss: 1.050 | Val loss: 1.151 | Gen: ethay airay ongiongionginay isway orwwwsgway\n","Epoch:  47 | Train loss: 0.961 | Val loss: 1.079 | Gen: ethay airway ongingiongioway isway orwwwwway\n","Epoch:  48 | Train loss: 1.022 | Val loss: 1.081 | Gen: ethay airway ongingiongioway isway orwwfheway\n","Epoch:  49 | Train loss: 0.992 | Val loss: 1.052 | Gen: ethay airway ongiongiotioway isway orwwgweway\n","Epoch:  50 | Train loss: 0.924 | Val loss: 1.050 | Gen: ethay auay ongcingcgcenay isway orwllllay\n","Epoch:  51 | Train loss: 0.877 | Val loss: 1.006 | Gen: ethay airway ongingiongioway isway orwfingay\n","Epoch:  52 | Train loss: 0.862 | Val loss: 1.010 | Gen: ethay airway ongctiongcioway isway orwwfhfay\n","Epoch:  53 | Train loss: 0.834 | Val loss: 0.986 | Gen: ethay airay ongcecececioway isway orwwfly\n","Epoch:  54 | Train loss: 0.811 | Val loss: 0.962 | Gen: ethay airway ongcingciongway isway orwwwwway\n","Epoch:  55 | Train loss: 0.789 | Val loss: 0.939 | Gen: ethay airway ongcecongconay isway oringwgway\n","Epoch:  56 | Train loss: 0.778 | Val loss: 0.910 | Gen: ethay airway ongcecongcecay isway orwwwwway\n","Epoch:  57 | Train loss: 0.750 | Val loss: 0.898 | Gen: ethay airway ongcecececioway isway orwwwwway\n","Epoch:  58 | Train loss: 0.737 | Val loss: 0.904 | Gen: ethay airway ongcingciongway isway orwwgwlay\n","Epoch:  59 | Train loss: 0.761 | Val loss: 0.909 | Gen: ethay airway ongcingcioncay isway orly-oray\n","Epoch:  60 | Train loss: 0.811 | Val loss: 0.941 | Gen: ethay airway ongcecioneceway isway oringwinay\n","Epoch:  61 | Train loss: 0.752 | Val loss: 0.925 | Gen: ethay airway ongcingcioneway isway orwwwwway\n","Epoch:  62 | Train loss: 0.753 | Val loss: 0.955 | Gen: ethay airway ongcecionecinway isway oringwway\n","Epoch:  63 | Train loss: 0.754 | Val loss: 0.950 | Gen: ethay airway ondindiondioway isway orwwly-way\n","Epoch:  64 | Train loss: 0.729 | Val loss: 0.885 | Gen: ethay airway ongciveciongway isway orwwly-way\n","Epoch:  65 | Train loss: 0.733 | Val loss: 0.946 | Gen: ethay airway ondingctiong isway orlllllay\n","Epoch:  66 | Train loss: 0.783 | Val loss: 0.933 | Gen: ethay airway onditionctioway isway orwwwwway\n","Epoch:  67 | Train loss: 0.799 | Val loss: 0.983 | Gen: ethay airway ongcecececay isway orwwwwway\n","Epoch:  68 | Train loss: 0.826 | Val loss: 0.938 | Gen: ethay airway ondititay-onay isway orwwwwway\n","Epoch:  69 | Train loss: 0.770 | Val loss: 0.888 | Gen: ethay airway oncungcececeway isway oriringway\n","Epoch:  70 | Train loss: 0.798 | Val loss: 0.900 | Gen: ethay airway ongcececececay isway orlingway\n","Epoch:  71 | Train loss: 0.762 | Val loss: 0.922 | Gen: ethay airway ongcengcengcay isway oringwiway\n","Epoch:  72 | Train loss: 0.716 | Val loss: 0.873 | Gen: ethay airway onditiongcunay isway orwwwsiway\n","Epoch:  73 | Train loss: 0.707 | Val loss: 0.849 | Gen: ethay airway onditiongcioway isway orwwwwlay\n","Epoch:  74 | Train loss: 0.699 | Val loss: 0.870 | Gen: ethay airway onditiongcecay isway orwskwsay\n","Epoch:  75 | Train loss: 0.668 | Val loss: 0.855 | Gen: ethay airway onditingcechay isway orwwwwway\n","Epoch:  76 | Train loss: 0.655 | Val loss: 0.835 | Gen: ethay airway onditivecionay isway orlllllay\n","Epoch:  77 | Train loss: 0.666 | Val loss: 0.856 | Gen: ethay airway ondingctiongway isway orkwwsway\n","Epoch:  78 | Train loss: 0.651 | Val loss: 0.838 | Gen: ethay airway onditiongctiway isway oringwway\n","Epoch:  79 | Train loss: 0.727 | Val loss: 0.866 | Gen: ethay airway onditionengcay isway orswwwway\n","Epoch:  80 | Train loss: 0.706 | Val loss: 0.858 | Gen: ethay airway ondiviviongcay isway orkwkwway\n","Epoch:  81 | Train loss: 0.704 | Val loss: 0.815 | Gen: ethay airway onditiongcecay isway orfwwwway\n","Epoch:  82 | Train loss: 0.651 | Val loss: 0.801 | Gen: ethay airway onditiongcenay isway orwwwwway\n","Epoch:  83 | Train loss: 0.636 | Val loss: 0.822 | Gen: ethay airway onditiongctiway isway orwwwwway\n","Epoch:  84 | Train loss: 0.648 | Val loss: 0.828 | Gen: ethay airway onditingctioway isway orlklkway\n","Epoch:  85 | Train loss: 0.787 | Val loss: 1.016 | Gen: ettay airway ongctioniongway isway orlllllay\n","Epoch:  86 | Train loss: 0.868 | Val loss: 0.963 | Gen: ettay airway ongctiongctifay isway orlllllay\n","Epoch:  87 | Train loss: 0.785 | Val loss: 0.917 | Gen: ettay airway ongengiongioway isway orlllllay\n","Epoch:  88 | Train loss: 0.720 | Val loss: 0.890 | Gen: ettay airway ongctionenghay isway orlllllay\n","Epoch:  89 | Train loss: 0.697 | Val loss: 0.855 | Gen: ettay airway onditioniongway isway orlllllay\n","Epoch:  90 | Train loss: 0.677 | Val loss: 0.857 | Gen: ethay airway onditiongionay isway orlllllay\n","Epoch:  91 | Train loss: 0.697 | Val loss: 0.903 | Gen: ethay airway ondiongiongiy isway orlly-oway\n","Epoch:  92 | Train loss: 0.678 | Val loss: 0.874 | Gen: ethay airway ondiongiongiway isway orlllllay\n","Epoch:  93 | Train loss: 0.649 | Val loss: 0.832 | Gen: ethay airway ondiongionghay isway orlllllay\n","Epoch:  94 | Train loss: 0.628 | Val loss: 0.827 | Gen: ethay airway ondiongctionay isway orlllllay\n","Epoch:  95 | Train loss: 0.622 | Val loss: 0.820 | Gen: ethay airway ondiongiongifay isway orlllllay\n","Epoch:  96 | Train loss: 0.613 | Val loss: 0.822 | Gen: ehtay airway ondctiongionay isway orlllllay\n","Epoch:  97 | Train loss: 0.626 | Val loss: 0.813 | Gen: ethay airway ondctiongionay isway orly-inway\n","Epoch:  98 | Train loss: 0.604 | Val loss: 0.803 | Gen: ethay airway ondctiongionay isway orlllllay\n","Epoch:  99 | Train loss: 0.596 | Val loss: 0.819 | Gen: ethay airway onditiongionay isway orlllllay\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway onditiongionay isway orlllllay\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ULCMHm5ZF7vx","colab_type":"code","outputId":"1d276f24-bed1-494c-d801-de260921c7ae","executionInfo":{"status":"error","timestamp":1575664722093,"user_tz":300,"elapsed":897,"user":{"displayName":"Li Chen","photoUrl":"","userId":"08452733754503056217"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["TEST_SENTENCE = 'the math building is large'\n","translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-45-2d44b828ea8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTEST_SENTENCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'the math building is large'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"source:\\t\\t{} \\ntranslated:\\t{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'transformer_encoder' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"qbfZCByITOI6","colab_type":"text"},"source":["# Attention visualization"]},{"cell_type":"code","metadata":{"id":"itCGMv3FdXsn","colab_type":"code","colab":{}},"source":["TEST_WORD_ATTN = 'street'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xBv4QQuBiU-V","colab_type":"text"},"source":["## Visualize RNN attention map"]},{"cell_type":"code","metadata":{"id":"aXvqoQYONMTA","colab_type":"code","colab":{}},"source":["visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuOvxfA1NMz3","colab_type":"text"},"source":["## Visualize transformer attention maps from all the transformer layers"]},{"cell_type":"code","metadata":{"id":"HSSB4wd8-M7g","colab_type":"code","colab":{}},"source":["visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"],"execution_count":0,"outputs":[]}]}